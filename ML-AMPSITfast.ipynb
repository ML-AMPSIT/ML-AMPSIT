{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ef7a6543fc46de87a791787d1e12ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n        <div style=\\'text-align:center; font-family:\"Segoe UI\", sans-serif; font-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "  \n",
    "importance_list=[]\n",
    "first_ord=[]\n",
    "score_list = []\n",
    "pvalue_list =[]\n",
    "mse_list = []\n",
    "mae_list = []\n",
    "ypred=[]\n",
    "ytest=[]\n",
    "\n",
    "import json\n",
    "\n",
    "with open('E:/newGRASS/VARStxt/configAMPSIT.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "totalhours = config['totalhours']\n",
    "variables = config['variables']\n",
    "regions = config['regions']\n",
    "verticalmax = config['verticalmax']\n",
    "totalsim = config['totalsim']\n",
    "parameter_names = config['parameter_names']\n",
    "output_path = config['output_pathname']\n",
    "tun_iter = config['tun_iter']\n",
    "    \n",
    "#############################################################################################         \n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def sa_randomforest(X_train, X_test, y_train, y_test, importance_list,score_list,pvalue_list,mse_list,mae_list,ytest,ypred,f,tun):\n",
    "  from sklearn.ensemble import RandomForestRegressor\n",
    "  from joblib import dump, load\n",
    "\n",
    "  if tun==1:\n",
    "\n",
    "    from skopt import BayesSearchCV\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    #params = {\n",
    "    #    'n_estimators': (10, 100),  \n",
    "    #    'max_depth': (2, 20),  \n",
    "    #    'min_samples_split': (2, 10), \n",
    "    #    'min_samples_leaf': (1, 10),  \n",
    "    #    'max_features': ['sqrt', 'log2', None],  \n",
    "    #    'bootstrap': [True, False],  \n",
    "    #}\n",
    "\n",
    "  \n",
    "    params = {\n",
    "        'n_estimators': (10, 30),  \n",
    "        'max_depth': (2, 6),  \n",
    "        'min_samples_split': (2, 10), \n",
    "        'min_samples_leaf': (1, 8),  \n",
    "        'max_features': [None],  \n",
    "        'bootstrap': [True],  \n",
    "    }\n",
    "\n",
    "\n",
    "    opt = BayesSearchCV(rf, params, n_iter=tun_iter, cv=5, n_jobs=-1)\n",
    "    opt.fit(X_train, y_train)\n",
    "    \n",
    "    score = cross_val_score(opt, X_train, y_train, cv=5).mean()\n",
    "\n",
    "    if os.path.exists(f'{output_path}tuning_results_rf_{f[:-4]}.txt'):\n",
    "        os.remove(f'{output_path}tuning_results_rf_{f[:-4]}.txt')\n",
    "    with open(f'{output_path}tuning_results_rf_{f[:-4]}.txt', 'a') as file:\n",
    "        file.write(\"Best parameters: {}\\n\".format(opt.best_params_))\n",
    "        file.write(\"Cross-validation score: {}\\n\".format(score))\n",
    "\n",
    "\n",
    "    best_params = opt.get_params()\n",
    "    \n",
    "    best_rf_params = {key.replace(\"estimator__\", \"\"): value for key, value in best_params.items() if key.startswith('estimator__')}\n",
    "    rf = RandomForestRegressor(**best_rf_params)\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    importances = rf.feature_importances_\n",
    "\n",
    "    dump(rf, output_path+'rf_model_'+f[:-4]+'.joblib')\n",
    "    \n",
    "  else:\n",
    "    \n",
    "    if tun==2:\n",
    "      loaded_model = load(output_path+'rf_model_'+f[:-4]+'.joblib')\n",
    "      \n",
    "      y_pred = loaded_model.predict(X_test)\n",
    "      \n",
    "      importances = loaded_model.feature_importances_\n",
    "      \n",
    "    else:            \n",
    "\n",
    "      rf = RandomForestRegressor(n_estimators=20, max_depth=5, max_features='log2',min_samples_leaf= 1, min_samples_split= 2)\n",
    "    \n",
    "      rf.fit(X_train, y_train)\n",
    "      \n",
    "      y_pred = rf.predict(X_test)\n",
    "\n",
    "      importances = rf.feature_importances_\n",
    "\n",
    "\n",
    "  if len(importance_list) <= totalhours-1:\n",
    "      importance_list.append(importances)\n",
    "  else:\n",
    "      importance_list = []               \n",
    "  \n",
    "  spearman_corr, p_value = spearmanr(y_test, y_pred)\n",
    "  score=spearman_corr\n",
    "  pvalue= p_value\n",
    "  \n",
    "  from sklearn.metrics import mean_squared_error\n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  from sklearn.metrics import mean_absolute_error\n",
    "  mae = mean_absolute_error(y_test, y_pred)\n",
    "  \n",
    "  if len(score_list) <= totalhours-1:\n",
    "    score_list.append(score)\n",
    "    pvalue_list.append(pvalue)\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "  else:\n",
    "    score_list=[]\n",
    "    pvalue_list=[]\n",
    "    mse_list=[]\n",
    "    mae_list=[]\n",
    "\n",
    "  if len(ytest)<=totalhours-1:\n",
    "    ytest.append(y_test)\n",
    "    ypred.append(y_pred)\n",
    "  else:\n",
    "    ytest=[]\n",
    "    ypred=[]\n",
    "\n",
    "\n",
    "def sa_xgboost(X_train, X_test, y_train, y_test, importance_list,score_list,pvalue_list,mse_list,mae_list,ytest,ypred,f,tun):\n",
    "  from xgboost import XGBRFRegressor\n",
    "  from joblib import dump, load\n",
    "\n",
    "  \n",
    "  if tun==1:\n",
    "\n",
    "    from skopt import BayesSearchCV\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    gb = XGBRFRegressor()\n",
    "\n",
    "    params = {\n",
    "        'n_estimators': (10, 30),\n",
    "        'max_depth': (2, 6),\n",
    "    }\n",
    "    '''\n",
    "    from skopt.space import Real, Integer, Categorical\n",
    "    params = {\n",
    "        'n_estimators': Integer(50, 100),  # Numero di alberi\n",
    "        'max_depth': Integer(3, 10),  # Profondità massima di ciascun albero\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),  # Tasso di apprendimento\n",
    "        'min_child_weight': Integer(1, 10),  # Minimo peso richiesto per un nodo\n",
    "        'gamma': Real(0, 0.5),  # Minima riduzione della perdita necessaria per fare ulteriori partizioni\n",
    "        'subsample': Real(0.5, 1.0),  # Frazione di esempi da usare per addestrare ciascun albero\n",
    "        'colsample_bytree': Real(0.5, 1.0),  # Frazione di feature da usare per costruire ciascun albero\n",
    "        'reg_lambda': Real(1e-2, 1e2, prior='log-uniform'),  # Regolarizzazione L2 sui pesi\n",
    "        'reg_alpha': Real(1e-2, 1e2, prior='log-uniform'),  # Regolarizzazione L1 sui pesi\n",
    "    }\n",
    "    '''\n",
    "    opt = BayesSearchCV(gb, params, n_iter=tun_iter, cv=5, n_jobs=-1)\n",
    "\n",
    "    opt.fit(X_train, y_train)\n",
    "    score = cross_val_score(opt, X_train, y_train, cv=5).mean()\n",
    "\n",
    "    if os.path.exists(f'{output_path}tuning_results_xgb_{f[:-4]}.txt'):\n",
    "        os.remove(f'{output_path}tuning_results_xgb_{f[:-4]}.txt')\n",
    "    with open(f'{output_path}tuning_results_xgb_{f[:-4]}.txt', 'a') as file:\n",
    "        file.write(\"Best parameters: {}\\n\".format(opt.best_params_))\n",
    "        file.write(\"Cross-validation score: {}\\n\".format(score))\n",
    "\n",
    "    best_params = opt.get_params()\n",
    "    gb = XGBRFRegressor(**best_params)\n",
    "\n",
    "    gb.fit(X_train, y_train)\n",
    "  \n",
    "    y_pred = gb.predict(X_test)\n",
    "    \n",
    "    importances = gb.feature_importances_\n",
    "\n",
    "    dump(gb, output_path+'xgb_model_'+f[:-4]+'.joblib')\n",
    "    \n",
    "  else:\n",
    "\n",
    "    if tun==2:\n",
    "      loaded_model = load(output_path+'xgb_model_'+f[:-4]+'.joblib')\n",
    "      \n",
    "      y_pred = loaded_model.predict(X_test)\n",
    "      \n",
    "      importances = loaded_model.feature_importances_\n",
    "      \n",
    "    else:        \n",
    "    \n",
    "      gb = XGBRFRegressor(n_estimators=20, max_depth=5, max_features='log2',min_samples_leaf= 1, min_samples_split= 2)\n",
    "    \n",
    "      gb.fit(X_train, y_train)\n",
    "    \n",
    "      y_pred = gb.predict(X_test)\n",
    "\n",
    "      importances = gb.feature_importances_\n",
    "\n",
    "  if len(importance_list) <= totalhours-1:\n",
    "      importance_list.append(importances)\n",
    "  else:\n",
    "      importance_list = []               \n",
    "  \n",
    "  spearman_corr, p_value = spearmanr(y_test, y_pred)\n",
    "  score=spearman_corr\n",
    "  pvalue= p_value\n",
    "  \n",
    "  from sklearn.metrics import mean_squared_error\n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  from sklearn.metrics import mean_absolute_error\n",
    "  mae = mean_absolute_error(y_test, y_pred)\n",
    "  \n",
    "  if len(score_list) <= totalhours-1:\n",
    "    score_list.append(score)\n",
    "    pvalue_list.append(pvalue)\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "  else:\n",
    "    score_list=[]\n",
    "    pvalue_list=[]\n",
    "    mse_list=[]\n",
    "    mae_list=[]\n",
    "\n",
    "  if len(ytest)<=totalhours-1:\n",
    "    ytest.append(y_test)\n",
    "    ypred.append(y_pred)\n",
    "  else:\n",
    "    ytest=[]\n",
    "    ypred=[]\n",
    "\n",
    "\n",
    "def sa_cart(X_train, X_test, y_train, y_test, importance_list,score_list,pvalue_list,mse_list,mae_list,ytest,ypred,f,tun):\n",
    "  from sklearn.tree import DecisionTreeRegressor\n",
    "  \n",
    "  from joblib import dump, load\n",
    "  \n",
    "  if tun==1:\n",
    "\n",
    "    from skopt import BayesSearchCV\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    dt = DecisionTreeRegressor()\n",
    "\n",
    "    params = {\n",
    "        'max_depth': (2, 6),\n",
    "        'min_samples_split': (2, 10),\n",
    "        'min_samples_leaf': (1, 5)\n",
    "    }\n",
    "\n",
    "    opt = BayesSearchCV(dt, params, n_iter=tun_iter, cv=5, n_jobs=-1)\n",
    "\n",
    "    opt.fit(X_train, y_train)\n",
    "    score = cross_val_score(opt, X_train, y_train, cv=5).mean()\n",
    "\n",
    "    if os.path.exists(f'{output_path}tuning_results_cart_{f[:-4]}.txt'):\n",
    "        os.remove(f'{output_path}tuning_results_cart_{f[:-4]}.txt')\n",
    "    with open(f'{output_path}tuning_results_cart_{f[:-4]}.txt', 'a') as file:\n",
    "        file.write(\"Best parameters: {}\\n\".format(opt.best_params_))\n",
    "        file.write(\"Cross-validation score: {}\\n\".format(score))\n",
    "\n",
    "    best_params = opt.get_params()\n",
    "    \n",
    "    dt_params = {key: value for key, value in best_params.items() if key in ['max_depth', 'min_samples_split', 'min_samples_leaf']}\n",
    "\n",
    "    dt = DecisionTreeRegressor(**dt_params)\n",
    "\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = dt.predict(X_test)\n",
    "\n",
    "    importances = dt.feature_importances_\n",
    "\n",
    "    dump(dt, output_path+'cart_model_'+f[:-4]+'.joblib')\n",
    "    \n",
    "  else:\n",
    "    \n",
    "    if tun==2:\n",
    "      loaded_model = load(output_path+'cart_model_'+f[:-4]+'.joblib')\n",
    "      \n",
    "      y_pred = loaded_model.predict(X_test)\n",
    "      \n",
    "      importances = loaded_model.feature_importances_\n",
    "      \n",
    "    else:    \n",
    "    \n",
    "      dt = DecisionTreeRegressor(max_depth=5,min_samples_leaf= 1, min_samples_split= 2)\n",
    "    \n",
    "      dt.fit(X_train, y_train)\n",
    "\n",
    "      y_pred = dt.predict(X_test)\n",
    "\n",
    "      importances = dt.feature_importances_\n",
    "\n",
    "\n",
    "  if len(importance_list) <= totalhours-1:\n",
    "      importance_list.append(importances)\n",
    "  else:\n",
    "      importance_list = []               \n",
    "  \n",
    "  spearman_corr, p_value = spearmanr(y_test, y_pred)\n",
    "  score=spearman_corr\n",
    "  pvalue= p_value\n",
    "  \n",
    "  from sklearn.metrics import mean_squared_error\n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  from sklearn.metrics import mean_absolute_error\n",
    "  mae = mean_absolute_error(y_test, y_pred)\n",
    "  \n",
    "  if len(score_list) <= totalhours-1:\n",
    "    score_list.append(score)\n",
    "    pvalue_list.append(pvalue)\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "  else:\n",
    "    score_list=[]\n",
    "    pvalue_list=[]\n",
    "    mse_list=[]\n",
    "    mae_list=[]\n",
    "\n",
    "  if len(ytest)<=totalhours-1:\n",
    "    ytest.append(y_test)\n",
    "    ypred.append(y_pred)\n",
    "  else:\n",
    "    ytest=[]\n",
    "    ypred=[]\n",
    "\n",
    "\n",
    "\n",
    "def sa_lassoregression(X_train, X_test, y_train, y_test, importance_list,score_list,pvalue_list,mse_list,mae_list,ytest,ypred,f,tun):\n",
    "  import numpy as np\n",
    "  from sklearn.linear_model import LassoCV\n",
    "  from joblib import dump, load\n",
    "  \n",
    "  if tun==1:\n",
    "    \n",
    "    from skopt import BayesSearchCV\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    lasso_cv = LassoCV()\n",
    "\n",
    "    params = {\n",
    "        'eps': (1e-8, 1e-1, 'log-uniform'),\n",
    "        'n_alphas': (50, 300),\n",
    "        'tol': (1e-8, 1e-3, 'log-uniform'),\n",
    "        'cv': [5, 7],\n",
    "    }\n",
    "\n",
    "    opt = BayesSearchCV(lasso_cv, params, n_iter=tun_iter, cv=5, n_jobs=-1)\n",
    "\n",
    "    opt.fit(X_train, y_train)\n",
    "\n",
    "    score = cross_val_score(opt, X_train, y_train, cv=5).mean()\n",
    "\n",
    "    if os.path.exists(f'{output_path}tuning_results_lasso_{f[:-4]}.txt'):\n",
    "        os.remove(f'{output_path}tuning_results_lasso_{f[:-4]}.txt')\n",
    "    with open(f'{output_path}tuning_results_lasso_{f[:-4]}.txt', 'a') as file:\n",
    "        file.write(\"Best parameters: {}\\n\".format(opt.best_params_))\n",
    "        file.write(\"Cross-validation score: {}\\n\".format(score))\n",
    "\n",
    "    y_pred = opt.best_estimator_.predict(X_test)\n",
    "\n",
    "    importances = np.abs(opt.best_estimator_.coef_)\n",
    "\n",
    "    dump(opt, output_path+'lasso_model_'+f[:-4]+'.joblib')\n",
    "\n",
    "  else:\n",
    "  \n",
    "    if tun==2:\n",
    "      loaded_model = load(output_path+'lasso_model_'+f[:-4]+'.joblib')\n",
    "      \n",
    "      y_pred = loaded_model.predict(X_test)\n",
    "      \n",
    "      importances = np.abs(loaded_model.best_estimator_.coef_)\n",
    "      \n",
    "    else:\n",
    "  \n",
    "  \n",
    "      lasso_cv = LassoCV(cv=5)\n",
    "      \n",
    "      lasso_cv.fit(X_train, y_train)                        \n",
    "\n",
    "      y_pred = lasso_cv.predict(X_test)\n",
    "      \n",
    "      importances = np.abs(lasso_cv.coef_)\n",
    "                      \n",
    "  \n",
    "       \n",
    "  if len(importance_list) <= totalhours-1:\n",
    "      #importance_list.append(importances)\n",
    "      sum_importances = np.sum(importances, axis=0)\n",
    "      normalized_importances = importances / sum_importances       \n",
    "      importance_list.append(normalized_importances)\n",
    "  else:\n",
    "      importance_list = []\n",
    "\n",
    "  spearman_corr, p_value = spearmanr(y_test, y_pred)\n",
    "  score=spearman_corr\n",
    "  pvalue= p_value\n",
    "  \n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  mae = mean_absolute_error(y_test, y_pred)\n",
    "  \n",
    "  if len(score_list) <= totalhours-1:\n",
    "    score_list.append(score)\n",
    "    pvalue_list.append(pvalue)\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "  else:\n",
    "    score_list=[]\n",
    "    pvalue_list=[]\n",
    "    mse_list=[]\n",
    "    mae_list=[]\n",
    "\n",
    "  if len(ytest)<=totalhours-1:\n",
    "    ytest.append(y_test)\n",
    "    ypred.append(y_pred)\n",
    "  else:\n",
    "    ytest=[]\n",
    "    ypred=[]\n",
    "\n",
    "def sa_svm(X_train, X_test, y_train, y_test, problem, N, importance_list,score_list,pvalue_list,mse_list,mae_list,ytest,ypred,f,tun):\n",
    "  from sklearn import svm\n",
    "  from joblib import dump, load\n",
    "  from sklearn.model_selection import train_test_split, cross_val_score \n",
    "  \n",
    "  \n",
    "  if tun==1:\n",
    "    \n",
    "    from skopt import BayesSearchCV\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    svm_model = svm.SVR()\n",
    "    \n",
    "    #params = {\n",
    "    #    'C': (1e-3, 1e+1, 'log-uniform'),\n",
    "    #    'kernel': ['linear'],\n",
    "    #    'gamma': (1e-3, 1e+1, 'log-uniform'),\n",
    "    #    'epsilon': (1e-4, 1e-1, 'log-uniform'), \n",
    "    #}\n",
    "    \n",
    "    from skopt.space import Real, Categorical, Integer\n",
    "    params = {\n",
    "        'C': Real(1e-4, 1e+1, prior='log-uniform'), \n",
    "        'kernel': Categorical(['poly']),\n",
    "        'gamma': Real(1e-3, 1e+1, prior='log-uniform'),\n",
    "        'epsilon': Real(1e-4, 1e-1, prior='log-uniform'),\n",
    "        'degree': Integer(2, 6),\n",
    "        'coef0': Real(0, 10),\n",
    "    }\n",
    "\n",
    "\n",
    "    svr = BayesSearchCV(svm_model, params, n_iter=tun_iter, cv=5, n_jobs=-1)\n",
    "\n",
    "    svr.fit(X_train, y_train)\n",
    "\n",
    "    score = cross_val_score(svr, X_train, y_train, cv=5).mean()\n",
    "\n",
    "    if os.path.exists(f'{output_path}tuning_results_svm_{f[:-4]}.txt'):\n",
    "        os.remove(f'{output_path}tuning_results_svm_{f[:-4]}.txt')\n",
    "    with open(f'{output_path}tuning_results_svm_{f[:-4]}.txt', 'a') as file:\n",
    "        file.write(\"Best parameters: {}\\n\".format(svr.best_params_))\n",
    "        file.write(\"Cross-validation score: {}\\n\".format(score))\n",
    "\n",
    "    y_pred = svr.predict(X_test)\n",
    "    #coef = svr.best_estimator_.coef_[0]  #classical linear\n",
    "    \n",
    "    #import shap\n",
    "    #best_svm_model = svr.best_estimator_\n",
    "    #explainer = shap.Explainer(best_svm_model.predict, X_train)\n",
    "    #shap_values = explainer(X_test)\n",
    "    #coef = np.abs(shap_values.values).mean(axis=0) #shap\n",
    "    \n",
    "    dump(svr, output_path+'svm_model_'+f[:-4]+'.joblib')\n",
    "  \n",
    "  else:\n",
    " \n",
    "    if tun==2:\n",
    "      svr = load(output_path+'svm_model_'+f[:-4]+'.joblib')\n",
    "      \n",
    "      y_pred = svr.predict(X_test)\n",
    "      \n",
    "      #coef=loaded_model.best_estimator_.coef_[0] #classical linear\n",
    "      \n",
    "    else:\n",
    "      \n",
    "      svr = svm.SVR(kernel='linear', C=1, epsilon=0.1)\n",
    "      svr.fit(X_train, y_train)\n",
    "      \n",
    "      y_pred = svr.predict(X_test)\n",
    "      \n",
    "      #coef=clf.coef_[0] #classical linear\n",
    "  \n",
    "  \n",
    "  from SALib.sample import sobol\n",
    "  X_sobol = sobol.sample(problem, N, calc_second_order=True)\n",
    "  from numpy import zeros\n",
    "  Y_sobol = zeros((len(X_sobol), 1))\n",
    "  for i in range(len(X_sobol)):\n",
    "      Y_sobol[i] =svr.predict(X_sobol[i].reshape(1, -1))\n",
    "  Y_sobol = Y_sobol.reshape(-1)\n",
    "  from SALib.analyze import sobol\n",
    "  Si = sobol.analyze(problem, Y_sobol, calc_second_order=True, print_to_console=False)\n",
    "  importances = np.where(Si['ST'] < 1, Si['ST'], np.nan) #sobol\n",
    "  \n",
    "  \n",
    "  if len(importance_list) <= totalhours-1:\n",
    "      #importance_list.append(coef) #classical linear / shap\n",
    "      #importance_list.append(importances) #sobol\n",
    "      sum_importances = np.sum(importances, axis=0)\n",
    "      normalized_importances = importances / sum_importances       \n",
    "      importance_list.append(normalized_importances)\n",
    "  else:\n",
    "      importance_list = []\n",
    "\n",
    "  spearman_corr, p_value = spearmanr(y_test, y_pred)\n",
    "  score=spearman_corr\n",
    "  pvalue= p_value\n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  mae = mean_absolute_error(y_test, y_pred)\n",
    "  \n",
    "  if len(score_list) <= totalhours-1:\n",
    "    score_list.append(score)\n",
    "    pvalue_list.append(pvalue)\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "  else:\n",
    "    score_list=[]\n",
    "    pvalue_list=[]\n",
    "    mse_list=[]\n",
    "    mae_list=[]\n",
    "\n",
    "  if len(ytest)<=totalhours-1:\n",
    "    ytest.append(y_test)\n",
    "    ypred.append(y_pred)\n",
    "  else:\n",
    "    ytest=[]\n",
    "    ypred=[]\n",
    "\n",
    "\n",
    "def sa_baesyanreg(X_train, X_test, y_train, y_test,problem,N, importance_list,score_list,pvalue_list,mse_list,mae_list,ytest,ypred,first_ord,f,tun):\n",
    "  from sklearn.model_selection import train_test_split\n",
    "\n",
    "  from sklearn.linear_model import BayesianRidge\n",
    "  from joblib import dump, load\n",
    "\n",
    "\n",
    "  if tun==1:\n",
    "    \n",
    "    from skopt import BayesSearchCV\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    br_model = BayesianRidge()\n",
    "\n",
    "    params = {\n",
    "        'max_iter': (100, 500),  \n",
    "        'tol': (1e-9, 1e-3, 'log-uniform'),\n",
    "        'alpha_1': (1e-10, 1e-4, 'log-uniform'),  \n",
    "        'alpha_2': (1e-10, 1e-4, 'log-uniform'),  \n",
    "        'lambda_1': (1e-10, 1e-4, 'log-uniform'),  \n",
    "        'lambda_2': (1e-10, 1e-4, 'log-uniform'), \n",
    "        'fit_intercept': [True, False], \n",
    "    }\n",
    "\n",
    "\n",
    "    br = BayesSearchCV(br_model, params, n_iter=tun_iter, cv=5, n_jobs=-1)\n",
    "\n",
    "    br.fit(X_train, y_train)\n",
    "\n",
    "    score = cross_val_score(br, X_train, y_train, cv=5).mean()\n",
    "    \n",
    "    if os.path.exists(f'{output_path}tuning_results_br_{f[:-4]}.txt'):\n",
    "        os.remove(f'{output_path}tuning_results_br_{f[:-4]}.txt')\n",
    "    with open(f'{output_path}tuning_results_br_{f[:-4]}.txt', 'a') as file:\n",
    "        file.write(\"Best parameters: {}\\n\".format(br.best_params_))\n",
    "        file.write(\"Cross-validation score: {}\\n\".format(score))\n",
    "    \n",
    "    dump(br, output_path+'br_model_'+f[:-4]+'.joblib')\n",
    "    y_pred = br.predict(X_test)\n",
    "    \n",
    "  else:    \n",
    "    if tun==2:\n",
    "      br = load(output_path+'br_model_'+f[:-4]+'.joblib')\n",
    "      y_pred = br.predict(X_test)\n",
    "    else:\n",
    "      br = BayesianRidge()\n",
    "      br.fit(X_train, y_train)\n",
    "      \n",
    "      y_pred = br.predict(X_test)\n",
    "\n",
    "\n",
    "  from SALib.sample import sobol\n",
    "  X_sobol = sobol.sample(problem, N, calc_second_order=True)\n",
    "  from numpy import zeros\n",
    "  Y_sobol = zeros((len(X_sobol), 1))\n",
    "  for i in range(len(X_sobol)):\n",
    "      Y_sobol[i] = br.predict(X_sobol[i].reshape(1, -1))\n",
    "  Y_sobol = Y_sobol.reshape(-1)\n",
    "  from SALib.analyze import sobol\n",
    "  Si = sobol.analyze(problem, Y_sobol, calc_second_order=True, print_to_console=False)\n",
    "\n",
    "  importances = Si['ST']\n",
    "  first_order = Si['S1']\n",
    "  interactions=Si['S2']\n",
    "  np.savetxt(output_path+f'interactions_matrix_{len(importance_list)}.txt', interactions, delimiter='\\t', fmt='%f')\n",
    "  \n",
    "  \n",
    "  if len(importance_list) <= totalhours-1:\n",
    "      #importance_list.append(importances)\n",
    "      sum_importances = np.sum(importances, axis=0)\n",
    "      normalized_importances = importances / sum_importances       \n",
    "      importance_list.append(normalized_importances)\n",
    "      first_ord.append(first_order)\n",
    "  else:\n",
    "      importance_list = []\n",
    "      first_ord=[]\n",
    "\n",
    "\n",
    "  from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "  spearman_corr, p_value = spearmanr(y_test, y_pred)\n",
    "  score=spearman_corr\n",
    "  pvalue= p_value  \n",
    "  \n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  mae = mean_absolute_error(y_test, y_pred)\n",
    "  \n",
    "  if len(score_list) <= totalhours-1:\n",
    "    score_list.append(score)\n",
    "    pvalue_list.append(pvalue)\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "  else:\n",
    "    score_list=[]\n",
    "    pvalue_list=[]\n",
    "    mse_list=[]\n",
    "    mae_list=[]\n",
    "\n",
    "  if len(ytest)<=totalhours-1:\n",
    "    ytest.append(y_test)\n",
    "    ypred.append(y_pred)\n",
    "  else:\n",
    "    ytest=[]\n",
    "    ypred=[] \n",
    "\n",
    "def sa_gaussianreg(X_train, X_test, y_train, y_test,problem,N, importance_list,score_list,pvalue_list,mse_list,mae_list,ytest,ypred,first_ord,f,tun):\n",
    "\n",
    "  from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "  from sklearn.gaussian_process.kernels import RBF\n",
    "  from sklearn.gaussian_process.kernels import Matern\n",
    "  from joblib import dump, load\n",
    "  \n",
    "  \n",
    "  if tun==1:\n",
    "    \n",
    "    from skopt import BayesSearchCV\n",
    "    from sklearn.model_selection import cross_val_score  \n",
    "    from skopt.space import Real, Categorical, Integer\n",
    "    from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, Sum, Product\n",
    "        \n",
    "    #kernel = RBF()    \n",
    "    #kernel = Matern()\n",
    "    kernel = RationalQuadratic()\n",
    "    #kernel = Sum(Matern(), RationalQuadratic())\n",
    "    #kernel = Product(Matern(), RationalQuadratic())\n",
    "    gp_model = GaussianProcessRegressor(kernel=kernel)\n",
    "    \n",
    "    '''\n",
    "    ##RBF\n",
    "    #params = {\n",
    "    #'alpha': (1e-10, 1e+3, 'log-uniform'),  \n",
    "    #'n_restarts_optimizer': (0, 20),\n",
    "    #'kernel__length_scale': (1e-2, 1e+2, 'log-uniform'),\n",
    "    #}\n",
    "    params = {\n",
    "        'alpha': Real(1e-8, 1e+3, prior='log-uniform'),\n",
    "        'n_restarts_optimizer': Integer(0, 20),\n",
    "        'kernel__length_scale': Real(1e-2, 1e+2, prior='log-uniform'),\n",
    "    }    \n",
    "    '''\n",
    "    ''\n",
    "    ##rational quadratic\n",
    "    params = {\n",
    "    'alpha': (1e-10, 1e+3, 'log-uniform'),  \n",
    "    'n_restarts_optimizer': (0, 20),\n",
    "    'kernel__length_scale': (1e-2, 1e+2, 'log-uniform'),\n",
    "    'kernel__alpha': Real(1e-2, 1e+2, prior='log-uniform'),\n",
    "    }\n",
    "    '''\n",
    "    ''\n",
    "    ##matern\n",
    "    params = {\n",
    "    'alpha': (1e-10, 1e+3, 'log-uniform'),  \n",
    "    'n_restarts_optimizer': (0, 20),\n",
    "    'kernel__length_scale': (1e-2, 1e+2, 'log-uniform'),\n",
    "    'kernel__nu': Real(1e-2, 1e+2, prior='log-uniform'),\n",
    "    }    \n",
    "    '''\n",
    "\n",
    "    ##product/sum\n",
    "    #params = {\n",
    "    #'alpha': Real(1e-10, 1e+3, prior='log-uniform'),\n",
    "    #'n_restarts_optimizer': Integer(0, 20),\n",
    "    #'kernel__k1__length_scale': Real(1e-2, 1e+2, prior='log-uniform'),  # Per Matern nel kernel Sum/Product\n",
    "    #'kernel__k2__length_scale': Real(1e-2, 1e+2, prior='log-uniform'),  # Per RationalQuadratic nel kernel Sum/Product\n",
    "    #'kernel__k1__nu': Real(0.5, 2.5),  # Parametro aggiuntivo per Matern\n",
    "    #'kernel__k2__alpha': Real(0.1, 10),  # Parametro aggiuntivo per RationalQuadratic\n",
    "    #}\n",
    "    \n",
    "    gp = BayesSearchCV(gp_model, params, n_iter=tun_iter, cv=5,n_jobs=-1)\n",
    "    gp.fit(X_train, y_train)\n",
    "\n",
    "    score = cross_val_score(gp, X_train, y_train, cv=5).mean()\n",
    "\n",
    "    if os.path.exists(f'{output_path}tuning_results_gp_{f[:-4]}.txt'):\n",
    "        os.remove(f'{output_path}tuning_results_gp_{f[:-4]}.txt')\n",
    "    with open(f'{output_path}tuning_results_gp_{f[:-4]}.txt', 'a') as file:\n",
    "        file.write(\"Best parameters: {}\\n\".format(gp.best_params_))\n",
    "        file.write(\"Cross-validation score: {}\\n\".format(score))\n",
    "    \n",
    "    dump(gp, output_path+'gp_model_'+f[:-4]+'.joblib')\n",
    "    \n",
    "    y_pred = gp.predict(X_test)\n",
    "        \n",
    "  else:  \n",
    "    \n",
    "    if tun==2:\n",
    "      gp = load(output_path+'gp_model_'+f[:-4]+'.joblib')\n",
    "      y_pred = gp.predict(X_test)\n",
    "    else:\n",
    "      kernel = RBF(length_scale=1.0) ##########################################################\n",
    "      gp = GaussianProcessRegressor(kernel=kernel)\n",
    "      gp.fit(X_train, y_train)\n",
    "      \n",
    "      y_pred = gp.predict(X_test)\n",
    "  \n",
    "  \n",
    "  from SALib.sample import sobol\n",
    "  X_sobol = sobol.sample(problem, N, calc_second_order=True)\n",
    "  from numpy import zeros\n",
    "  Y_sobol = zeros((len(X_sobol), 1))\n",
    "  for i in range(len(X_sobol)):\n",
    "      Y_sobol[i] = gp.predict(X_sobol[i].reshape(1, -1))\n",
    "  Y_sobol = Y_sobol.reshape(-1)\n",
    "  from SALib.analyze import sobol\n",
    "  Si = sobol.analyze(problem, Y_sobol, calc_second_order=True, print_to_console=False)\n",
    "\n",
    "  importances = np.where(Si['ST'] < 1, Si['ST'], np.nan)\n",
    "  first_order=Si['S1']\n",
    "  interactions=Si['S2']\n",
    "  np.savetxt(output_path+f'interactions_matrix_{len(importance_list)}.txt', interactions, delimiter='\\t', fmt='%f')\n",
    "  \n",
    "  if len(importance_list) <= totalhours-1:\n",
    "      #importance_list.append(importances)\n",
    "      sum_importances = np.sum(importances, axis=0)\n",
    "      normalized_importances = importances / sum_importances       \n",
    "      importance_list.append(normalized_importances)\n",
    "      first_ord.append(first_order)\n",
    "  else:\n",
    "      importance_list = []\n",
    "      first_ord=[]\n",
    "\n",
    "  from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "  \n",
    "  spearman_corr, p_value = spearmanr(y_test, y_pred)\n",
    "  score=spearman_corr\n",
    "  pvalue= p_value  \n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  mae = mean_absolute_error(y_test, y_pred)\n",
    "  \n",
    "  if len(score_list) <= totalhours-1:\n",
    "    score_list.append(score)\n",
    "    pvalue_list.append(pvalue)\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "  else:\n",
    "    score_list=[]\n",
    "    pvalue_list=[]\n",
    "    mse_list=[]\n",
    "    mae_list=[]\n",
    "\n",
    "  if len(ytest)<=totalhours-1:\n",
    "    ytest.append(y_test)\n",
    "    ypred.append(y_pred)\n",
    "  else:\n",
    "    ytest=[]\n",
    "    ypred=[]\n",
    "\n",
    "  \n",
    "\n",
    "#############################################################################################\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def create_predict_and_plot_global(meth,N,var,hpoint,vpoint,hour,tun, n_sample, figsize=(10,6)):\n",
    "  def predict_and_plot(**x_dict):\n",
    "\n",
    "    if var >= 1 and var <= len(variables):\n",
    "        nam1 = variables[var - 1]\n",
    "    else:\n",
    "        nam1 = 'Invalid var value'\n",
    "\n",
    "    if hpoint >= 1 and hpoint <= len(regions):\n",
    "        nam2 = regions[hpoint - 1]\n",
    "    else:\n",
    "        nam2 = 'Invalid hpoint value'        \n",
    "\n",
    "    name=nam1+'_'+nam2+'_lev'+str(vpoint)\n",
    "    file_list=[nam1+'_'+nam2+'_lev'+str(vpoint)+'_'+str(i)+'.txt' for i in range(1,totalhours+1)]\n",
    "\n",
    "    Xnonscaled = np.loadtxt(output_path+'X.txt') \n",
    "\n",
    "\n",
    "    for file in file_list:\n",
    "\n",
    "      f=file\n",
    "      ynonscaled = np.loadtxt(output_path+file, delimiter=',')\n",
    "      \n",
    "      \n",
    "      Xnonscaled = Xnonscaled[:N, :]\n",
    "      ynonscaled = ynonscaled[:N]\n",
    "\n",
    "      min_len = min(len(Xnonscaled), len(ynonscaled))\n",
    "      Xnonscaled = Xnonscaled[:min_len, :]\n",
    "      ynonscaled = ynonscaled[:min_len]\n",
    "           \n",
    "      y=ynonscaled\n",
    "      X=Xnonscaled              \n",
    "\n",
    "      from sklearn.model_selection import train_test_split\n",
    "      \n",
    "      partition=0.3\n",
    "        \n",
    "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=partition, random_state=42)\n",
    "\n",
    "      from sklearn.preprocessing import StandardScaler\n",
    "      from joblib import dump\n",
    "\n",
    "      scalerX = StandardScaler()\n",
    "      X_train = scalerX.fit_transform(X_train)\n",
    "      X_test = scalerX.transform(X_test)\n",
    "\n",
    "      scalery = StandardScaler()\n",
    "      y_train = scalery.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "      y_test = scalery.transform(y_test.reshape(-1, 1)).ravel()\n",
    "      \n",
    "      ''\n",
    "      Xlow = np.min(X_train, axis=0)\n",
    "      Xup = np.max(X_train, axis=0)\n",
    "      Nn = n_sample \n",
    "      bounds = [(Xlow[i], Xup[i]) for i in range(Xlow.shape[0])]\n",
    "      problem = {'num_vars': X.shape[1], 'names': parameter_names, 'bounds': bounds}            \n",
    "      ''\n",
    "\n",
    "      if meth==1:\n",
    "        Si = sa_randomforest(X_train, X_test, y_train, y_test, importance_list,score_list,pvalue_list,mse_list,mae_list,ytest,ypred,f,tun)\n",
    "        method='randomforest'\n",
    "      elif meth==2:\n",
    "        Si = sa_lassoregression(X_train, X_test, y_train, y_test, importance_list,score_list,pvalue_list,mse_list,mae_list,ytest,ypred,f,tun)\n",
    "        method='lasso'\n",
    "      elif meth==3:\n",
    "        Si = sa_svm(X_train, X_test, y_train, y_test,problem,Nn, importance_list,score_list,pvalue_list,mse_list,mae_list,ytest,ypred,f,tun)\n",
    "        method='svm'\n",
    "      elif meth==4:        \n",
    "        Si = sa_baesyanreg(X_train, X_test, y_train, y_test,problem,Nn, importance_list,score_list,pvalue_list,mse_list,mae_list,ytest,ypred,first_ord,f,tun)\n",
    "        method='br'\n",
    "      elif meth==5:\n",
    "        Si = sa_gaussianreg(X_train, X_test, y_train, y_test,problem,Nn, importance_list,score_list,pvalue_list,mse_list,mae_list,ytest,ypred,first_ord,f,tun)\n",
    "        method='gp'\n",
    "      elif meth==6:\n",
    "        Si = sa_xgboost(X_train, X_test, y_train, y_test, importance_list,score_list,pvalue_list,mse_list,mae_list,ytest,ypred,f,tun)\n",
    "        method='xgboost'              \n",
    "      elif meth==7:\n",
    "        Si = sa_cart(X_train, X_test, y_train, y_test, importance_list,score_list,pvalue_list,mse_list,mae_list,ytest,ypred,f,tun)\n",
    "        method='cart'\n",
    "        \n",
    "        \n",
    "      data = np.array([score_list, pvalue_list, mse_list, mae_list]).T\n",
    "      \n",
    "      import pandas as pd\n",
    "      df = pd.DataFrame(data, columns=['score', 'pvalue' , 'mse', 'mae'])\n",
    "      importance_df = pd.DataFrame(importance_list)\n",
    "      if hour==totalhours:\n",
    "        importance_df.to_csv(output_path+'importance'+method+str(N)+file[:-7]+'.txt', header=False, index=False, sep=' ')\n",
    "        df.to_csv(output_path+'df'+method+str(N)+file[:-7]+'.txt', header=False, index=False, sep=' ')\n",
    "\n",
    "#############################################################################################\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    matplotlib.rc('xtick',labelsize=10)\n",
    "    matplotlib.rc('ytick',labelsize=10)\n",
    "    \n",
    "    x=np.arange(totalhours)\n",
    "\n",
    "    pvalue = df.loc[hour-1, 'pvalue']\n",
    "    if pvalue < 0.0001:\n",
    "        pvalue_str = \"<0.0001\"\n",
    "    else:\n",
    "        pvalue_str = \"{:.3g}\".format(pvalue)\n",
    "\n",
    "    \n",
    "    font=12\n",
    "    \n",
    "    if meth in [4,5]:\n",
    "    \n",
    "      fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(12, 12))\n",
    "\n",
    "      x = np.arange(totalhours)\n",
    "      \n",
    "      from matplotlib.cm import get_cmap\n",
    "      cmap = get_cmap('viridis')\n",
    "      colors = [cmap(i / (len(parameter_names)-1)) for i in range(len(parameter_names))]\n",
    "      #colors = ['b','g','r','c','m','y']\n",
    "      #colors = ['#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7', '#999999']\n",
    "      for i in range(len(parameter_names)):     \n",
    "        axs[0,0].plot(x,[abs(importance_list[j][i]) for j in range(totalhours)],label=parameter_names[i],color=colors[i])  \n",
    "      \n",
    "      axs[0,0].set_xlabel(\"Time\",fontsize=12)\n",
    "      axs[0,0].set_ylabel(\"Importance\",fontsize=12)\n",
    "      axs[0,0].set_title('Importance time evolution',fontsize=12)\n",
    "      axs[0,0].legend(prop={'size':10, 'weight':'normal'}, ncol=4, loc='upper center', bbox_to_anchor=(0.5, 0.99))\n",
    "      axs[0,0].grid(True)\n",
    "\n",
    "      ln1 =axs[0,1].plot(df.index, df['score'], color=colors[0], label='Score')\n",
    "      ln2 =axs[0,1].plot(df.index, df['mse'], color=colors[1], label='MSE')\n",
    "      ln3 =axs[0,1].plot(df.index, df['mae'], color=colors[2], label='MAE')\n",
    "      twin=axs[0,1].twinx()\n",
    "      ln4 =twin.plot(df.index, df['pvalue'], color=colors[3], label='p-value')\n",
    "      twin.set_ylabel('p-value',fontsize=12)\n",
    "      twin.set_ylim(0, 1e-15)\n",
    "      axs[0,1].set_xlabel('Time',fontsize=12)\n",
    "      axs[0,1].set_ylabel('Value',fontsize=12)\n",
    "      axs[0,1].set_title('Metrics time evolution',fontsize=12)\n",
    "      axs[0,1].set_ylim(0, 1)\n",
    "      lns = ln1 + ln2 + ln3 + ln4\n",
    "      labels = [l.get_label() for l in lns]\n",
    "      axs[0,1].legend(lns,labels,prop={'size':10,'weight':'normal'},loc='upper center', bbox_to_anchor=(0.15, 0.8))\n",
    "      axs[0,1].grid(True)\n",
    "\n",
    "      axs[1,0].scatter(ytest[hour-1], ypred[hour-1])\n",
    "      ideal = [min(ytest[hour-1]), max(ytest[hour-1])]\n",
    "      axs[1,0].plot(ideal, ideal, 'r--')\n",
    "      axs[1,0].set_xlabel('True Values',fontsize=12)\n",
    "      axs[1,0].set_ylabel('Predictions',fontsize=12)\n",
    "      axs[1,0].set_title('Hour: {}; Performance (score: {:.2f} [p: {}], mse: {:.2f}, mae: {:.2f})'.format(hour-1, df.loc[hour-1, 'score'], pvalue_str, df.loc[hour-1, 'mse'], df.loc[hour-1, 'mae']))\n",
    "      axs[1,0].grid(True)\n",
    "\n",
    "      axs[1,1].bar(parameter_names,importance_list[hour-1])\n",
    "      axs[1,1].set_xticklabels(parameter_names, rotation=90, ha='right')\n",
    "      axs[1,1].set_xlabel(\"Features\",fontsize=12)\n",
    "      axs[1,1].set_ylabel(\"Coefficient\",fontsize=12)\n",
    "      axs[1,1].set_title('Hour: '+str(hour-1)+'; Sobol total index',fontsize=12)\n",
    "      axs[1,1].grid(True)\n",
    "\n",
    "      import seaborn as sns\n",
    "      file_path = os.path.join(output_path, f'interactions_matrix_{hour-1}.txt')\n",
    "      inter_mat = np.loadtxt(file_path, delimiter='\\t')\n",
    "      sns.heatmap(inter_mat, annot=True, cmap='coolwarm', fmt=\".3f\", cbar_kws={'label': 'Sobol second order index'},\n",
    "                  xticklabels=parameter_names, yticklabels=parameter_names, ax=axs[2, 0])\n",
    "      axs[2, 0].set_title(f'Interactions Matrix {hour-1}',fontsize=12)\n",
    "      \n",
    "      \n",
    "      axs[2,1].bar(parameter_names,first_ord[hour-1])\n",
    "      axs[2,1].set_xticklabels(parameter_names, rotation=90, ha='right')\n",
    "      axs[2,1].set_xlabel(\"Features\",fontsize=12)\n",
    "      axs[2,1].set_ylabel(\"Coefficient\",fontsize=12)\n",
    "      axs[2,1].set_title('Hour: '+str(hour-1)+'; Sobol first order index',fontsize=12)    \n",
    "      axs[2,1].grid(True)\n",
    "\n",
    "    else:\n",
    "      fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "\n",
    "      x = np.arange(totalhours)\n",
    "      \n",
    "      from matplotlib.cm import get_cmap\n",
    "      cmap = get_cmap('viridis')\n",
    "      \n",
    "      \n",
    "      colorsfill = plt.cm.tab10(np.linspace(0, 1, 6))\n",
    "      cmap = get_cmap('viridis')\n",
    "      colors = [cmap(i / (len(parameter_names)-1)) for i in range(len(parameter_names))]\n",
    "      for i in range(len(parameter_names)):    \n",
    "        axs[0,0].plot(x,[abs(importance_list[j][i]) for j in range(totalhours)],label=parameter_names[i],color=colors[i]) \n",
    "      \n",
    "      axs[0,0].set_xlabel(\"Time\",fontsize=12)\n",
    "      axs[0,0].set_ylabel(\"Importance\",fontsize=12)\n",
    "      axs[0,0].set_title('Importance time evolution',fontsize=12)\n",
    "      axs[0,0].set_ylim(0, 1)\n",
    "      axs[0,0].legend(prop={'size':10, 'weight':'normal'}, ncol=4, loc='upper center', bbox_to_anchor=(0.5, 0.99))\n",
    "      axs[0,0].grid(True)\n",
    "      \n",
    "      ln1 =axs[0,1].plot(df.index, df['score'], color=colors[0], label='Score')\n",
    "      ln2 =axs[0,1].plot(df.index, df['mse'], color=colors[1], label='MSE')\n",
    "      ln3 =axs[0,1].plot(df.index, df['mae'], color=colors[2], label='MAE')\n",
    "      twin=axs[0,1].twinx()\n",
    "      ln4 =twin.plot(df.index, df['pvalue'], color=colors[3], label='p-value')\n",
    "      twin.set_ylabel('p-value',fontsize=12)\n",
    "      twin.set_ylim(0, 1e-15)\n",
    "      axs[0,1].set_xlabel('Time',fontsize=12)\n",
    "      axs[0,1].set_ylabel('Value',fontsize=12)\n",
    "      axs[0,1].set_title('Metrics time evolution',fontsize=12)\n",
    "      axs[0,1].set_ylim(0, 1)\n",
    "      lns = ln1 + ln2 + ln3 + ln4\n",
    "      labels = [l.get_label() for l in lns]\n",
    "      axs[0,1].legend(lns,labels,prop={'size':10,'weight':'normal'},loc='upper center', bbox_to_anchor=(0.15, 0.8))\n",
    "      axs[0,1].grid(True)\n",
    "      \n",
    "      axs[1,0].scatter(ytest[hour-1], ypred[hour-1])\n",
    "      ideal = [min(ytest[hour-1]), max(ytest[hour-1])]\n",
    "      axs[1,0].plot(ideal, ideal, 'r--')\n",
    "      axs[1,0].set_xlabel('True Values',fontsize=12)\n",
    "      axs[1,0].set_ylabel('Predictions',fontsize=12)\n",
    "      axs[1,0].set_title('Hour: {}; Performance (score: {:.2f} [p: {}], mse: {:.2f}, mae: {:.2f})'.format(hour-1, df.loc[hour-1, 'score'], pvalue_str, df.loc[hour-1, 'mse'], df.loc[hour-1, 'mae']))\n",
    "      axs[1,0].grid(True)\n",
    "      \n",
    "      axs[1,1].bar(parameter_names,abs(importance_list[hour-1]))\n",
    "      \n",
    "      axs[1,1].set_xlabel(\"Features\",fontsize=12)\n",
    "      axs[1,1].set_xticklabels(parameter_names, rotation=90, ha='right')\n",
    "      axs[1,1].set_ylabel(\"Coefficient\",fontsize=12)\n",
    "      axs[1,1].set_title('Hour: '+str(hour-1)+'; Feature importance',fontsize=12)\n",
    "      axs[1,1].grid(True)\n",
    "      \n",
    "    fig.suptitle(method + ' on ' + name[:-9] + ' N:'+ str(N) + ' v:' + str(vpoint), fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path+ method+'_'+name[:-9] +'_hour'+str(hour-1)+ '_v'+str(vpoint)+'.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "  return predict_and_plot\n",
    "########################################################################\n",
    "\n",
    "def nearest_divisible(n , lst):\n",
    "  size = len(lst)\n",
    "  remainder = n % (size+2)\n",
    "  if remainder == 0:\n",
    "    return n\n",
    "  diff = size - remainder\n",
    "  return n + diff\n",
    "\n",
    "n_sample_calc = nearest_divisible(1000, parameter_names)\n",
    "\n",
    "choices=['vpoint','hpoint','variable','N_simul','method','hour','tuning']\n",
    "vpointmin=1;\n",
    "vpointmax=verticalmax;\n",
    "hpointmin=1;\n",
    "hpointmax=len(regions);\n",
    "varmin=1;\n",
    "varmax=len(variables);\n",
    "Nmin=totalsim*1/10;\n",
    "Nmax=totalsim;\n",
    "methmin=1;\n",
    "methmax=7; ###\n",
    "tunmin=0;\n",
    "tunmax=2;\n",
    "hourmin=1;\n",
    "hourmax=totalhours;\n",
    "Xmin=[vpointmin,hpointmin,varmin,Nmin,methmin,hourmin,tunmin]\n",
    "Xmax=[vpointmax,hpointmax,varmax,Nmax,methmax,hourmax,tunmax]\n",
    "Nstep=[verticalmax-1,hpointmax-1,varmax-1, (Nmax - Nmin) // 10, methmax-1, totalhours-1 , tunmax]\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def interactive_global(_, meth, N, var, hpoint, vpoint, hour, tun, with_button=False):\n",
    "    Xnew = [vpoint, hpoint, var, N, meth, hour, tun]\n",
    "    \n",
    "    sliders = {}\n",
    "    slider_widgets = []\n",
    "    label_widgets = []\n",
    "\n",
    "    for i, f in enumerate(choices):\n",
    "        SliderClass = widgets.IntSlider if f in ['vpoint', 'hpoint', 'variable', 'N_simul', 'method', 'hour', 'tuning'] else widgets.FloatSlider\n",
    "        slider = SliderClass(\n",
    "            value=int(Xnew[i]) if SliderClass == widgets.IntSlider else Xnew[i],\n",
    "            min=int(Xmin[i]) if SliderClass == widgets.IntSlider else Xmin[i],\n",
    "            max=int(Xmax[i]) if SliderClass == widgets.IntSlider else Xmax[i],\n",
    "            step=1 if SliderClass == widgets.IntSlider else (Xmax[i] - Xmin[i]) / Nstep[i],\n",
    "            layout=widgets.Layout(width='400px', margin='0 0 0 40px')\n",
    "        )\n",
    "        sliders[f] = slider\n",
    "        slider_widgets.append(slider)\n",
    "\n",
    "    label_texts = [\n",
    "        \"<div style='width:240px; text-align:right;'><b>Vertical levels:</b> {}</div>\".format(verticalmax),\n",
    "        \"<div style='width:240px; text-align:right;'><b>Domain horizontal point:</b><br>{}</div>\".format(', '.join(regions)),\n",
    "        \"<div style='width:240px; text-align:right;'><b>Variable:</b><br>{}</div>\".format(', '.join(variables)),\n",
    "        \"<div style='width:240px; text-align:right;'><b>Number of simulations:</b><br>Choose sample size</div>\",\n",
    "        \"<div style='width:240px; text-align:right;'><b>Methods:</b><br>1=RF, ..., 7=CART</div>\",\n",
    "        \"<div style='width:240px; text-align:right;'><b>Hour:</b><br>Select hour of interest</div>\",\n",
    "        \"<div style='width:240px; text-align:right;'><b>Tuning:</b><br>0=Off, 1=On, 2=Load</div>\"\n",
    "    ]\n",
    "\n",
    "    label_widgets = [widgets.HTML(txt) for txt in label_texts]\n",
    "\n",
    "    status_output = widgets.Output()\n",
    "\n",
    "\n",
    "    should_stop = {\"value\": False}\n",
    "\n",
    "    stop_button = widgets.Button(\n",
    "        description=\"STOP\",\n",
    "        layout=widgets.Layout(width='100px', height='45px'),\n",
    "        tooltip=\"Interrupt the process\",\n",
    "        button_style='danger'\n",
    "    )\n",
    "    stop_button.style.font_weight = 'bold'\n",
    "    stop_button.style.font_size = '16px'\n",
    "\n",
    "    def stop_execution(_):\n",
    "        should_stop[\"value\"] = True\n",
    "        status_output.clear_output()\n",
    "        with status_output:\n",
    "            display(widgets.HTML(\"<b style='color: red;'>Execution interrupted by user.</b>\"))\n",
    "\n",
    "    stop_button.on_click(stop_execution)\n",
    "\n",
    "\n",
    "    run_button = None\n",
    "    if with_button:\n",
    "        run_button = widgets.Button(\n",
    "            description=\"RUN\",\n",
    "            layout=widgets.Layout(width='180px', height='45px'),\n",
    "            tooltip=\"Run the model with selected configuration\"\n",
    "        )\n",
    "        run_button.style.button_color = '#0078D7'\n",
    "        run_button.style.font_weight = 'bold'\n",
    "        run_button.style.font_size = '16px'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def update_results_within(_):\n",
    "            should_stop[\"value\"] = False  # reset stop flag\n",
    "            status_output.clear_output()\n",
    "            with status_output:\n",
    "                display(widgets.HTML(\"<div style='color: green; font-weight: bold;'>Running model... Please wait.</div>\"))\n",
    "\n",
    "            # (slider values read here...)\n",
    "\n",
    "            if tun == 1:\n",
    "                with status_output:\n",
    "                    display(widgets.HTML(\"<i style='color: orange;'>Tuning is enabled — this might take a while...</i>\"))\n",
    "\n",
    "            try:\n",
    "                if should_stop[\"value\"]:\n",
    "                    raise RuntimeError(\"Process was interrupted by user.\")\n",
    "\n",
    "                on_change_global = create_predict_and_plot_global(\n",
    "                    meth=meth, N=N, var=var, hpoint=hpoint, vpoint=vpoint, hour=hour, tun=tun, n_sample=n_sample_calc\n",
    "                )\n",
    "\n",
    "                if should_stop[\"value\"]:\n",
    "                    raise RuntimeError(\"Process was interrupted by user.\")\n",
    "\n",
    "                on_change_global()\n",
    "\n",
    "                if should_stop[\"value\"]:\n",
    "                    raise RuntimeError(\"Process was interrupted by user.\")\n",
    "\n",
    "                with status_output:\n",
    "                    status_output.clear_output()\n",
    "                    display(widgets.HTML(\"<div style='color: blue; font-weight: bold;'>Output generated successfully.</div>\"))\n",
    "\n",
    "            except Exception as e:\n",
    "                with status_output:\n",
    "                    status_output.clear_output()\n",
    "                    display(widgets.HTML(f\"<b style='color: red;'>Error: {str(e)}</b>\"))\n",
    "\n",
    "        run_button.on_click(update_results_within)\n",
    "\n",
    "    # Pair labels with sliders\n",
    "    paired_rows = []\n",
    "    for label, slider in zip(label_widgets, slider_widgets):\n",
    "        row = widgets.HBox(\n",
    "            [label, slider],\n",
    "            layout=widgets.Layout(\n",
    "                justify_content='flex-start',\n",
    "                align_items='center',\n",
    "                padding='10px',\n",
    "                border='1px solid #ddd',\n",
    "                margin='6px 0',\n",
    "                border_radius='6px',\n",
    "                background_color='#f9f9f9',\n",
    "                gap='60px'\n",
    "            )\n",
    "        )\n",
    "        paired_rows.append(row)\n",
    "\n",
    "    sliders_and_labels_box = widgets.VBox(\n",
    "        paired_rows,\n",
    "        layout=widgets.Layout(\n",
    "            justify_content='center',\n",
    "            width='auto',\n",
    "            padding='10px'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # === Methods Legend ===\n",
    "    methods_legend = widgets.HTML(\n",
    "        \"\"\"\n",
    "        <div style=\"text-align: left; padding: 12px 20px; border: 1px solid #ccc; border-radius: 8px; background-color: #f4f4f4; font-size: 14px; line-height: 1.6; max-width: 280px;\">\n",
    "        <b>Available algorithms</b><br>\n",
    "        (1) Random Forest<br>\n",
    "        (2) Lasso<br>\n",
    "        (3) Support Vector Regression<br>\n",
    "        (4) Bayesian regression<br>\n",
    "        (5) Gaussian regression<br>\n",
    "        (6) XGBoost<br>\n",
    "        (7) CART\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    legend_and_sliders_box = widgets.HBox(\n",
    "        [sliders_and_labels_box, methods_legend],\n",
    "        layout=widgets.Layout(\n",
    "            justify_content='center',\n",
    "            align_items='flex-start',\n",
    "            gap='40px',\n",
    "            width='100%'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # === Headers ===\n",
    "    title = widgets.HTML(\"\"\"\n",
    "        <div style='text-align:center; font-family:\"Segoe UI\", sans-serif; font-size: 38px; font-weight: bold; color: #2c3e50; margin-top: 15px;'>\n",
    "            ML-AMPSIT\n",
    "        </div>\n",
    "    \"\"\")\n",
    "    subtitle = widgets.HTML(\"\"\"\n",
    "        <div style='text-align:center; font-family:Arial; font-size: 18px; color: #555;'>\n",
    "            Machine Learning-based Automated Multi-method Parameter Sensitivity and Importance analysis Tool\n",
    "        </div>\n",
    "    \"\"\")\n",
    "    subtitle2 = widgets.HTML(\"\"\"\n",
    "        <div style='text-align:center; font-size: 14px; color: #888;'>\n",
    "            Interactive UI by <b>Dario Di Santo</b>, Dept. of Civil, Environmental and Mechanical Engineering, University of Trento\n",
    "        </div>\n",
    "    \"\"\")\n",
    "    subtitle3 = widgets.HTML(\"\"\"\n",
    "        <div style='text-align:center; font-size: 14px; color: #666;'>\n",
    "            Configure the sliders and click RUN. The output will be saved as specified in <code>configAMPSIT.json</code>.<br>\n",
    "            Project: <a href='https://github.com/ML-AMPSIT/ML-AMPSIT' target='_blank'>GitHub Repository</a>\n",
    "        </div>\n",
    "    \"\"\")\n",
    "\n",
    "    # === Final UI ===\n",
    "    ui = widgets.VBox([\n",
    "        title, subtitle, subtitle2, subtitle3,\n",
    "        widgets.HTML('<hr>'),\n",
    "        legend_and_sliders_box,\n",
    "        widgets.HTML('<br>'),\n",
    "        widgets.HBox([run_button, stop_button], layout=widgets.Layout(justify_content='center', gap='20px')),\n",
    "        widgets.HTML('<br>'),\n",
    "        status_output\n",
    "    ], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "    return sliders, ui\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "on_change_global = create_predict_and_plot_global(\n",
    "    meth=1, N=totalsim, var=1, hpoint=1, vpoint=1, hour=totalhours, tun=0, n_sample=n_sample_calc\n",
    ")\n",
    "\n",
    "sliders, ui = interactive_global(on_change_global, 1, totalsim, 1, 1, 1, totalhours, 0, with_button=True)\n",
    "display(ui)\n",
    "\n",
    "# -----------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
